## Model selection

The results of the previous question show higher values in Spearman Correlation and Pearson Correlation for the LSTM model with allso a lower Test Loss (MSE). I chose to use the LSTM model to explore the attention, which uses mean pooling over the sequence as its aggregation mechanism and does not employ attention.

## Attention selection

We considered several attention variants before selecting Single-head Attention Pooling:

- Self-Attention: While powerful, self-attention is primarily a sequence-to-sequence operation. To obtain a single prediction value, we would still require a subsequent pooling step. Furthermore, for simple motif detection, modeling pairwise dependencies between every nucleotide is likely unnecessary compared to simply identifying the location of the motif.

- Multi-head Attention: While this allows the model to attend to different subspaces, it increases interpretability complexity.

- Cross-Attention: Standard encoder-decoder cross-attention is not applicable as we do not have a secondary conditioning sequence.

We chose Single-head attention pooling beacuse:
- Unlike static global pooling (mean or max), attention pooling learns a weighted sum of the hidden states. This allows the model to selectively aggregate information based on its relevance to the binding affinity, rather than treating all positions equally.
- A single attention head produces a unique probability distribution (via Softmax) over the input sequence. This acts as a direct map of feature importance (saliency), allowing us to easily visualize and validate if the model is focusing on known binding motifs.
- By limiting the architecture to a single head, we minimize the parameter count. This reduces the risk of overfitting (high variance), ensuring the model remains robust on smaller datasets where complex multi-head mechanisms might capture idiosyncratic noise.
- The mechanism acts as a bottleneck that filters out non-informative sequence positions (background noise), effectively emphasizing the hidden representations that correspond to the specific binding signal.
- Using a single head provides a clean comparison to standard RNN/CNN architectures, isolating the performance gain specifically to the re-weighting of temporal features without the confounding variables of multi-head projections.
- Given the short context window (41 nucleotides), the inductive bias of a single attention head, which assumes specific focal points drive the prediction, is sufficient to capture primary dependencies without the complexity of full self-attention blocks.

## Expected differences

1. Improved Feature Localization (Motif Identification)

Your primary expectation is that the model will shift from "averaging" the sequence to "selecting" the sequence.

    Behavior Change: Instead of the final prediction being an equal blend of all 41 nucleotides, the attention mechanism will allow the model to "attend" specifically to short, high-affinity motifs (like GCAUG).

    Impact: You expect the model to produce higher Spearman correlation scores because it can now distinguish between a sequence that has a perfect motif and one that just has similar "average" nucleotide composition but no specific binding site.

2. Higher Signal-to-Noise Ratio (SNR)

In the baseline model, "junk" RNA (nucleotides that don't contribute to binding) acts as a distractor that adds variance to the MSE.

    Behavior Change: The attention head acts as a learnable filter. By assigning near-zero weights to non-binding flanking regions, the model "denoises" the input before it reaches the final dense layers.

    Impact: This should result in a more stable Validation Loss and a lower overall MSE, as the model is no longer trying to find patterns in the random noise of the flanking sequences.

3. Faster and More Focused Convergence

Because the gradient can now flow directly to the most relevant parts of the sequence through the attention weights, the training dynamics should change.

    Behavior Change: You expect the Training Loss to decrease more sharply in the early epochs compared to the mean-pooling baseline.

    Impact: The model should reach its peak validation performance in fewer epochs. However, you should also watch for earlier overfitting; because the model is more "powerful," it might start memorizing specific training motifs faster, so you might need to monitor the gap between training and validation curves closely.

4. Qualitative Interpretability

Finally, you expect a change in how you can audit the model.

    Behavior Change: The model behavior becomes "explainable." You expect to be able to extract the attention vector and see clear "peaks" that correspond to known biological binding sites.

    Impact: This transforms the model from a "black box" into a tool for motif discovery, which is a significant behavioral upgrade for a biological deep learning task.

Suggested phrasing for the report:

    "We expect the incorporation of attention pooling to significantly enhance the model's spatial sensitivity. By replacing static mean pooling—which treats informative motifs and background noise equally—with a dynamic weighting mechanism, the model should demonstrate improved ranking ability (Spearman correlation). Specifically, we anticipate the attention weights will converge on known RBP binding sites, effectively acting as a supervised feature selector that stabilizes the regression task and provides a biologically interpretable justification for its predictions."